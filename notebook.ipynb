{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "inappropriate-purchase",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from dms2dec.dms_convert import dms2dec\n",
    "from shapely.geometry import Point, Polygon, LineString\n",
    "from shapely import wkt\n",
    "import pycountry\n",
    "import subprocess\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "import geopandas\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# PEX file used to package third party packages and ship it to spark\n",
    "# for more info visit https://databricks.com/blog/2020/12/22/how-to-manage-python-dependencies-in-pyspark.html\n",
    "os.environ['PYSPARK_PYTHON'] = \"./pyspark_pex_env.pex\"\n",
    "\n",
    "spark_session = SparkSession.builder \\\n",
    "        .master(\"spark://main:7077\") \\\n",
    "        .appName(\"yasser\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\",2)\\\n",
    "        .config(\"spark.cores.max\", 4)\\\n",
    "        .config(\"spark.driver.port\",9998)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .config(\"spark.files\", \"pyspark_pex_env.pex\").getOrCreate()\n",
    "\n",
    "\n",
    "context = spark_session.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-shoot",
   "metadata": {},
   "source": [
    "The notebook is tested on measurements extracted from here https://www.ecad.eu/dailydata/predefinedseries.php\n",
    "If you want to test it, make sure to remove the sources, elements, and stations files.\n",
    "\n",
    "You might noticed that some regions don't have any colors. This could mean that there are no stations that reside in that region in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "resident-permit",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEASUREMENT = 'TG'\n",
    "# needed countries\n",
    "# Sweden, Germany, Greece, Finland\n",
    "COUNTRIES_NEEDED = ['Sweden', 'Germany', 'Greece', 'Finland', \n",
    "                    'Iceland', 'Italy', 'Norway', 'Spain', 'Poland',\n",
    "                    'France']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "numerical-haiti",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = f\"hdfs://main:9000/user/ubuntu/{MEASUREMENT}\"\n",
    "PATH_TO_STATIONS = \"hdfs://main:9000/user/ubuntu/stations.txt\"\n",
    "\n",
    "# regex to get data in form of 111,222,aaaa,+aaa\n",
    "# \\+ is added to include the longitude and latitude\n",
    "regex_header = re.compile(r\"([A-Z\\s]*,){4,}\", re.IGNORECASE)\n",
    "regex_data = re.compile(r\"([\\d\\s\\+\\:-]*,){4,}\", re.IGNORECASE)\n",
    "regex_stations = re.compile(r\"([\\w\\s\\+\\:-]*,){4,}\", re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-young",
   "metadata": {},
   "source": [
    "# Process stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "historical-aaron",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_file = context.textFile(PATH_TO_STATIONS)\n",
    "\n",
    "# get stations_data\n",
    "stations_rows = stations_file.filter(lambda line: regex_stations.search(line) is not None)\\\n",
    "                 .map(lambda line: list(map(lambda x: x.strip(), line.split(\",\"))))\n",
    "\n",
    "# getting the data into a dataframe\n",
    "stations_header = stations_rows.first()\n",
    "df_stations = stations_rows.filter(lambda row: row != stations_header).toDF(stations_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "considered-sustainability",
   "metadata": {},
   "outputs": [],
   "source": [
    "# casting ID to int and taking needed stations\n",
    "countries_alpha_2 = list(map(lambda country: pycountry.countries.get(name=country).alpha_2, COUNTRIES_NEEDED))\n",
    "\n",
    "df_stations = df_stations.withColumn(\"STAID\", df_stations['STAID'].cast('int'))\n",
    "df_staitons_needed_countries = df_stations.filter(df_stations.CN.isin(countries_alpha_2))\\\n",
    "                               .drop(\"STANAME\", \"HGHT\", \"CN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fossil-accent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert from degree minutes seconds(DMS) to decimal\n",
    "def DMS_TO_DEC(dms):\n",
    "    return float(dms2dec(dms))\n",
    "\n",
    "udf_DMS_TO_DEC = udf(DMS_TO_DEC, StringType())\n",
    "\n",
    "df_stations_with_dms = df_staitons_needed_countries\\\n",
    "                                 .withColumn(\"LON\", udf_DMS_TO_DEC(\"LON\"))\\\n",
    "                                 .withColumn(\"LAT\", udf_DMS_TO_DEC(\"LAT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "south-durham",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# casting lon and lat to wkt for later usage \n",
    "def LON_LAT_TO_POINT(lon, lat):\n",
    "    return geopandas.points_from_xy([lon], [lat])[0].to_wkt()\n",
    "\n",
    "udf_TO_POINT = udf(LON_LAT_TO_POINT, StringType())\n",
    "\n",
    "df_stations_points = df_stations_with_dms.withColumn(\"point\", udf_TO_POINT(\"LON\", \"LAT\")).drop(\"LAT\", \"LON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "portable-teddy",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o193.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Master removed our application: KILLED\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3519)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3516)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-22abd4cf8e73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# getting list of needed stations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mneeded_STAID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_stations_points\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"STAID\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mneeded_STAID_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneeded_STAID\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTAID\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0mcolumn_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \"\"\"\n\u001b[1;32m    676\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o193.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Master removed our application: KILLED\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3519)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3516)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n"
     ]
    }
   ],
   "source": [
    "# getting list of needed stations\n",
    "needed_STAID = df_stations_points.select(\"STAID\").toPandas()\n",
    "needed_STAID_list = needed_STAID.STAID.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-camcorder",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-provider",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all measurement files\n",
    "data_files = context.wholeTextFiles(PATH_TO_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-negotiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files.repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-cleaners",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all files for needed stations\n",
    "data_files_needed = data_files.filter(lambda file_pair: int(re.findall(r'\\d+', file_pair[0])[1]) in needed_STAID_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-provider",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the line containing the data\n",
    "data_lines = data_files_needed.map(lambda x: x[1].split(\"\\r\\n\"))\\\n",
    "             .map(lambda line: list(filter(regex_data.match,line)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-comparative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the index of the measurement\n",
    "lines_of_single_file = data_files.first()[1].split(\"\\r\\n\")\n",
    "header_string = list(filter(regex_header.match, lines_of_single_file))[0]\n",
    "data_header = list(map(lambda x: x.strip(), header_string.split(',')))\n",
    "index_measurement = data_header.index(MEASUREMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-technique",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the average for each file with its corresponding station id\n",
    "def get_mean_for_file(lines):\n",
    "    lines_splited = list(map(lambda x: x.split(','), lines))\n",
    "\n",
    "    measurement_values = list(map(lambda x: int(x[index_measurement]),lines_splited))\n",
    "    measurement_values_valid = list(filter(lambda x: x != -9999, measurement_values))\n",
    "    mean = sum(measurement_values_valid) / len(lines_splited)\n",
    "    if MEASUREMENT == \"TG\":\n",
    "        mean *= 0.1\n",
    "    \n",
    "    return (int(lines_splited[0][0]), mean)\n",
    "\n",
    "data_formated = data_lines.map(get_mean_for_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-converter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only the id of the station and the the measurement is needed\n",
    "data_header_used = [\"STAID_\", \"AVG\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-audit",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = data_formated.toDF(data_header_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-royalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# include the mean measurement in each station\n",
    "df_stations_data = df_stations_points.join(df_data, df_stations_points.STAID == df_data.STAID_, 'inner').drop(\"STAID_\", \"STAID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-pierce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download shape files from https://www.diva-gis.org/gData and store them in shape_files\n",
    "PATH_TO_SHAPE_FILES = \"./shape_files\"\n",
    "countries_alpha_3 = map(lambda country: pycountry.countries.get(name=country).alpha_3, COUNTRIES_NEEDED)\n",
    "countries_alpha_3_need_download = list(filter(lambda country: not os.path.exists(f\"{PATH_TO_SHAPE_FILES}/{country}\"), countries_alpha_3))\n",
    "\n",
    "for alpha_3 in countries_alpha_3_need_download:\n",
    "    subprocess.run(f'wget http://biogeo.ucdavis.edu/data/diva/adm/{alpha_3}_adm.zip &&'\n",
    "                   f' mkdir -p {PATH_TO_SHAPE_FILES}/{alpha_3} &&'\n",
    "                   f' unzip {alpha_3}_adm.zip -d {PATH_TO_SHAPE_FILES}/{alpha_3} &&'\n",
    "                   f' rm {alpha_3}_adm.zip'\n",
    "                   , shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-credits",
   "metadata": {},
   "outputs": [],
   "source": [
    "# include all the shape files in one dataframe\n",
    "geo_df = pd.DataFrame()\n",
    "for country in COUNTRIES_NEEDED:\n",
    "    alpha_3 = pycountry.countries.get(name=country).alpha_3\n",
    "    geo_df = geo_df.append(geopandas.read_file(f\"{PATH_TO_SHAPE_FILES}/{alpha_3}/{alpha_3}_adm1.shp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-responsibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data from spark to be processed\n",
    "pandas_df_stations = df_stations_data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-trade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the region for each station\n",
    "gdf_data = geopandas.GeoDataFrame(pandas_df_stations['AVG'], geometry=list(map(lambda x: wkt.loads(x), pandas_df_stations['point'])), crs=\"EPSG:4326\")\n",
    "geo_df_index_ID_1 = geo_df.set_index(\"ID_1\")\n",
    "gdf_data['ID_1'] = -1\n",
    "for index1, gdf in gdf_data.iterrows():\n",
    "    for index2, geo in geo_df.iterrows():\n",
    "        if geo['geometry'].contains(gdf.geometry):\n",
    "            gdf_data.at[index1, 'ID_1'] = geo['ID_1']\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-holly",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df_joined = gdf_data.drop(['geometry'], axis=1).merge(geo_df, on=\"ID_1\", how='outer')\n",
    "index_names = geo_df_joined[ (geo_df_joined['ID_1'] == -1)].index \n",
    "\n",
    "geo_df_joined.drop(index_names, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-visit",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df_index_ID_1 = geo_df.set_index(\"ID_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-intent",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df_index_ID_1[\"AVG\"] = geo_df_joined.groupby(by=['ID_1']).mean().AVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-chess",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for country in COUNTRIES_NEEDED:    \n",
    "    geo_df_index_ID_1[geo_df_index_ID_1.NAME_0 == country].plot(column='AVG', cmap='OrRd', edgecolor='k', legend=True, figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-composite",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-arkansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "print(f\"It took {end_time - start_time} to finish the app\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
