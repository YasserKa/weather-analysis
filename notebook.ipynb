{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "inappropriate-purchase",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from dms2dec.dms_convert import dms2dec\n",
    "from shapely.geometry import Point, Polygon, LineString\n",
    "from shapely import wkt\n",
    "import pycountry\n",
    "import subprocess\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "import geopandas\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# PEX file used to package third party packages and ship it to spark\n",
    "# for more info visit https://databricks.com/blog/2020/12/22/how-to-manage-python-dependencies-in-pyspark.html\n",
    "os.environ['PYSPARK_PYTHON'] = \"./pyspark_pex_env.pex\"\n",
    "\n",
    "spark_session = SparkSession.builder \\\n",
    "        .master(\"spark://main:7077\") \\\n",
    "        .appName(\"yasser\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\",2)\\\n",
    "        .config(\"spark.cores.max\", 2)\\\n",
    "        .config(\"spark.driver.port\",9998)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .config(\"spark.files\", \"pyspark_pex_env.pex\").getOrCreate()\n",
    "\n",
    "context = spark_session.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-shoot",
   "metadata": {},
   "source": [
    "The notebook is tested on measurements extracted from here https://www.ecad.eu/dailydata/predefinedseries.php\n",
    "If you want to test it, make sure to remove the sources, elements, and stations files.\n",
    "\n",
    "You might noticed that some regions don't have any colors. This could mean that there are no stations that reside in that region in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "resident-permit",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEASUREMENT = 'TG'\n",
    "# needed countries\n",
    "# Sweden, Germany, Greece, Finland\n",
    "COUNTRIES_NEEDED = ['Sweden', 'Germany', 'Greece', 'Finland']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "numerical-haiti",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = f\"hdfs://main:9000/user/ubuntu/{MEASUREMENT}\"\n",
    "PATH_TO_STATIONS = \"hdfs://main:9000/user/ubuntu/stations.txt\"\n",
    "\n",
    "# regex to get data in form of 111,222,aaaa,+aaa\n",
    "# \\+ is added to include the longitude and latitude\n",
    "regex_header = re.compile(r\"([A-Z\\s]*,){4,}\", re.IGNORECASE)\n",
    "regex_data = re.compile(r\"([\\d\\s\\+\\:-]*,){4,}\", re.IGNORECASE)\n",
    "regex_stations = re.compile(r\"([\\w\\s\\+\\:-]*,){4,}\", re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-young",
   "metadata": {},
   "source": [
    "# Process stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "historical-aaron",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_file = context.textFile(PATH_TO_STATIONS)\n",
    "\n",
    "# get stations_data\n",
    "stations_rows = stations_file.filter(lambda line: regex_stations.search(line) is not None)\\\n",
    "                 .map(lambda line: list(map(lambda x: x.strip(), line.split(\",\"))))\n",
    "\n",
    "# getting the data into a dataframe\n",
    "stations_header = stations_rows.first()\n",
    "df_stations = stations_rows.filter(lambda row: row != stations_header).toDF(stations_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "considered-sustainability",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'alpha_2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e028f14fa785>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# casting ID to int and taking needed stations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcountries_alpha_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mcountry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpycountry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcountries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcountry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCOUNTRIES_NEEDED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_stations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_stations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"STAID\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_stations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'STAID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_staitons_needed_countries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_stations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_stations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountries_alpha_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-e028f14fa785>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(country)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# casting ID to int and taking needed stations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcountries_alpha_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mcountry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpycountry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcountries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcountry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCOUNTRIES_NEEDED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_stations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_stations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"STAID\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_stations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'STAID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_staitons_needed_countries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_stations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_stations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountries_alpha_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'alpha_2'"
     ]
    }
   ],
   "source": [
    "# casting ID to int and taking needed stations\n",
    "countries_alpha_2 = list(map(lambda country: pycountry.countries.get(name=country).alpha_2, COUNTRIES_NEEDED))\n",
    "\n",
    "df_stations = df_stations.withColumn(\"STAID\", df_stations['STAID'].cast('int'))\n",
    "df_staitons_needed_countries = df_stations.filter(df_stations.CN.isin(countries_alpha_2))\\\n",
    "                               .drop(\"STANAME\", \"HGHT\", \"CN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-accent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert from degree minutes seconds(DMS) to decimal\n",
    "def DMS_TO_DEC(dms):\n",
    "    return float(dms2dec(dms))\n",
    "\n",
    "udf_DMS_TO_DEC = udf(DMS_TO_DEC, StringType())\n",
    "\n",
    "df_stations_with_dms = df_staitons_needed_countries\\\n",
    "                                 .withColumn(\"LON\", udf_DMS_TO_DEC(\"LON\"))\\\n",
    "                                 .withColumn(\"LAT\", udf_DMS_TO_DEC(\"LAT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-durham",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# casting lon and lat to wkt for later usage \n",
    "def LON_LAT_TO_POINT(lon, lat):\n",
    "    return geopandas.points_from_xy([lon], [lat])[0].to_wkt()\n",
    "\n",
    "udf_TO_POINT = udf(LON_LAT_TO_POINT, StringType())\n",
    "\n",
    "df_stations_points = df_stations_with_dms.withColumn(\"point\", udf_TO_POINT(\"LON\", \"LAT\")).drop(\"LAT\", \"LON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-teddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting list of needed stations\n",
    "needed_STAID = df_stations_points.select(\"STAID\").toPandas()\n",
    "needed_STAID_list = needed_STAID.STAID.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-camcorder",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-provider",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all measurement files\n",
    "data_files = context.wholeTextFiles(PATH_TO_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-negotiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files.repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-cleaners",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all files for needed stations\n",
    "data_files_needed = data_files.filter(lambda file_pair: int(re.findall(r'\\d+', file_pair[0])[1]) in needed_STAID_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-provider",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the line containing the data\n",
    "data_lines = data_files_needed.map(lambda x: x[1].split(\"\\r\\n\"))\\\n",
    "             .map(lambda line: list(filter(regex_data.match,line)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-comparative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the index of the measurement\n",
    "lines_of_single_file = data_files.first()[1].split(\"\\r\\n\")\n",
    "header_string = list(filter(regex_header.match, lines_of_single_file))[0]\n",
    "data_header = list(map(lambda x: x.strip(), header_string.split(',')))\n",
    "index_measurement = data_header.index(MEASUREMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-technique",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the average for each file with its corresponding station id\n",
    "def get_mean_for_file(lines):\n",
    "    lines_splited = list(map(lambda x: x.split(','), lines))\n",
    "\n",
    "    measurement_values = list(map(lambda x: int(x[index_measurement]),lines_splited))\n",
    "    measurement_values_valid = list(filter(lambda x: x != -9999, measurement_values))\n",
    "    mean = sum(measurement_values_valid) / len(lines_splited)\n",
    "    if MEASUREMENT == \"TG\":\n",
    "        mean *= 0.1\n",
    "    \n",
    "    return (int(lines_splited[0][0]), mean)\n",
    "\n",
    "data_formated = data_lines.map(get_mean_for_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-converter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only the id of the station and the the measurement is needed\n",
    "data_header_used = [\"STAID_\", \"AVG\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-audit",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = data_formated.toDF(data_header_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-royalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# include the mean measurement in each station\n",
    "df_stations_data = df_stations_points.join(df_data, df_stations_points.STAID == df_data.STAID_, 'inner').drop(\"STAID_\", \"STAID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-pierce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download shape files from https://www.diva-gis.org/gData and store them in shape_files\n",
    "PATH_TO_SHAPE_FILES = \"./shape_files\"\n",
    "countries_alpha_3 = map(lambda country: pycountry.countries.get(name=country).alpha_3, COUNTRIES_NEEDED)\n",
    "countries_alpha_3_need_download = list(filter(lambda country: not os.path.exists(f\"{PATH_TO_SHAPE_FILES}/{country}\"), countries_alpha_3))\n",
    "\n",
    "for alpha_3 in countries_alpha_3_need_download:\n",
    "    subprocess.run(f'wget http://biogeo.ucdavis.edu/data/diva/adm/{alpha_3}_adm.zip &&'\n",
    "                   f' mkdir -p {PATH_TO_SHAPE_FILES}/{alpha_3} &&'\n",
    "                   f' unzip {alpha_3}_adm.zip -d {PATH_TO_SHAPE_FILES}/{alpha_3} &&'\n",
    "                   f' rm {alpha_3}_adm.zip'\n",
    "                   , shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-credits",
   "metadata": {},
   "outputs": [],
   "source": [
    "# include all the shape files in one dataframe\n",
    "geo_df = pd.DataFrame()\n",
    "for country in COUNTRIES_NEEDED:\n",
    "    alpha_3 = pycountry.countries.get(name=country).alpha_3\n",
    "    geo_df = geo_df.append(geopandas.read_file(f\"{PATH_TO_SHAPE_FILES}/{alpha_3}/{alpha_3}_adm2.shp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-responsibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data from spark to be processed\n",
    "pandas_df_stations = df_stations_data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-intro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# include the analysed data to the geo dataframe\n",
    "geo_df_joined = geo_df.merge(pandas_df_stations, how='cross')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-anderson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if each station is contained in each region and aggregate them\n",
    "stations_geo = geopandas.GeoDataFrame(geometry=list(map(lambda x: wkt.loads(x), geo_df_joined['point'])), crs=\"EPSG:4326\")\n",
    "geo_df_joined = geo_df_joined[geo_df_joined.geometry.contains(stations_geo)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-visit",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df_index_ID_2 = geo_df.set_index(\"ID_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-intent",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df_index_ID_2[\"AVG\"] = geo_df_joined.groupby(by=['ID_2']).mean().AVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-chess",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for country in COUNTRIES_NEEDED:    \n",
    "    geo_df_index_ID_2[geo_df_index_ID_2.NAME_0 == country].plot(column='AVG', cmap='OrRd', edgecolor='k', legend=True, figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-composite",
   "metadata": {},
   "outputs": [],
   "source": [
    "context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-arkansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "print(f\"It took {end_time - start_time} to finish the app\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
